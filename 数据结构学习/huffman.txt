Informally, an algorithm is any welldefined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output. An algorithm is thus a sequence of computational steps that transform the input into the output.
We can also view an algorithm as a tool for solving a well-specified computational problem. The statement of the problem specifies in general terms the desired input/output relationship. The algorithm describes a specific computational procedure for achieving that input/output relationship.
An algorithm is said to be correct if, for every input instance, it halts with the correct output. We say that a correct algorithm solves the given computational problem. An incorrect algorithm might not halt at all on some input instances, or it might halt with an answer other than the desired one. Contrary to what one might expect, incorrect algorithms can sometimes be useful, if their error rate can be controlled. We shall see an example of this in Chapter when we study algorithms for finding large prime numbers. Ordinarily, however, we shall be concerned only with correct algorithms.
Algorithms for optimization problems typically go through a sequence of steps, with a set of choices at each step. For many optimization problems, using dynamic programming to determine the best choices is overkill; simpler, more efficient algorithms will do. A greedy algorithm always makes the choice that looks best at the moment. That is, it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution. This chapter explores optimization problems that are solvable by greedy algorithms. 
Greedy algorithms do not always yield optimal solutions, but for many problems they do. We shall first examine in Section 16.1 a simple but nontrivial problem, the activity-selection problem, for which a greedy algorithm efficiently computes a solution. We shall arrive at the greedy algorithm by first considering a dynamic-programming solution and then showing that we can always make greedy choices to arrive at an optimal solution. Section 16.2 reviews the basic elements of the greedy approach, giving a more direct approach to proving greedy algorithms correct than the dynamic-programming-based process of Section 16.1. Section 16.3 presents an important application of greedy techniques: the design of data-compression (Huffman) codes. In Section 16.4, we investigate some of the theory underlying combinatorial structures called "matroids" for which a greedy algorithm always produces an optimal solution. Finally, Section 16.5 illustrates the application of matroids using a problem of scheduling unit-time tasks with deadlines and penalties. The greedy method is quite powerful and works well for a wide range of problems. Later chapters will present many algorithms that can be viewed as applications of the greedy method, including minimum-spanning-tree algorithms , Dijkstra's algorithm for shortest paths from a single source, and Chv¡äatal's greedy set-covering heuristic. Minimum-spanning-tree algorithms are a classic example of the greedy method. 
A greedy algorithm obtains an optimal solution to aproblem by making asequence of choices. For each decision point in thealgorithm, the choice that seems best at the moment is chosen. This heuristic strategy does not alwaysproduce an optimal solution,but as we saw in the activity-selection problem, sometimes it does. This section discusses some of the general properties of greedy methods. The process that we followed in Section 16.1 to develop agreedy algorithm was a bit more involved than is typical.
We went through the following steps:
(1) Determine the optimal substructure of the problem.
(2)Develop a recursive solution.
(3)Prove that at any stage of the recursion, one of the optimal choices is the greedy choice. Thus, it is always safe to make the greedy choice.
(4)Show that all but one of the subproblems induced by having made the greedy choice are empty.
(5)Develop a recursive algorithm that implements the greedy strategy.
(6)Convert the recursive algorithm to an iterative algorithm.
In going through these steps, we saw in great detail the dynamic-programming underpinnings of a greedy algorithm. In practice, however, we usually streamline the above steps when designing agreedy algorithm. We develop our substructure with an eye toward making a greedy choice that leaves just one subproblem to solve optimally. 
Greedy-choice property:
The first key ingredient is the greedy-choice property: aglobally optimal solution can be arrived at by making a locally optimal (greedy) choice. In other words, when we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems. Here is where greedy algorithms differ from dynamic programming. In dynamic programming, we make achoice at each step, but the choice usually depends on the solutions to subproblems. Consequently, we typically solve dynamic-programming problems in a bottom-up manner,progressing from smaller subproblems to larger subproblems. In a greedy algorithm, we make whatever choice seems best at the moment and then solve the subproblem arising after thechoice ismade. The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems. Thus, unlike dynamic programming, which solves the subproblems bottom up, a greedy strategy usually 
proesses in a top-down fashion, making one greedy choice after another, reducing each given problem instance to a smaller one. 
Of course, we must prove that a greedy choice at each step yields a globally optimal solution, and this is where cleverness may be required. 
Optimal substructure: 
A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. This property is a key ingredient of assessing the applicability of dynamic programming as well as greedy algorithms. 
We usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms. As mentioned above, we have the luxury of assuming that we arrived at a subproblem by having made the greedy choice in the original problem. All we really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem. This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step produces an optimal solution.
Greedy versus dynamic programming:
Because the optimal-substructure property is exploited by both the greedy and dynamic-programming strategies, one might be tempted to generate a dynamic-programming solution to a problem when a greedy solution suffices, or one might mistakenly think that a greedy solution works when in fact a dynamic-programming solution is required. To illustrate the subtleties between the two techniques, let us investigate two variants of a classical optimization problem.
Huffman codes:
Huffman codes are awidelyused and very effective technique for compressing data; savings of 20% to 90% are typical, depending on the characteristics of the data being compressed. We consider the data to be a sequence of characters. Huffman's greedy algorithm uses a table of the frequencies of occurrence of the characters to build up an optimal way of representing each character asa binary string.
Suppose we have a100,000-character data file that we wish to store compactly. We observe that the characters in the file occur with the frequencies given by Figure 16.3. That is, only six different characters appear, and the character aoccurs 45,000 times.

Constructing aHuffman code:
Huffman invented a greedy algorithm that constructs an optimal prefix code called a Huffmancode. Keeping in line with our observations in Section 16.2, its proof of correctness relies on the greedy-choice property and optimal substructure. Rather than demonstrating that these properties hold and then developing pseudocode, we present the pseudocode first. Doing so will help clarify how the algorithm makes greedy choices.
In the pseudocode that follows, we assume that C is a set of n characters and that each character c ¡Ê Cis an object with a defined frequency f[c].The algorithm builds the tree T corresponding to the optimal code in a bottom-up manner. It begins with a set of |C| leaves and performs a sequence of |C|-1 "merging" operations to create the final tree. A min-priority queue Q , keyed on f , is used to identify the two least-frequent objects to merge together. The result of the merger of two objects is a new object whose frequency is the sum of the frequencies of the two objects that were merged.

Correctness of Huffman's algorithm:
To prove that the greedy algorithm HUFFMAN is correct, we show that the problem of determining an optimal prefix code exhibits the greedy-choice and optimal-substructure properties. The next lemma shows that the greedy-choice property holds.
Lemma 16.2:
Let C be an alphabet in which each character c¡ÊC has frequency f[c]. Let x and y be two characters in C having the lowest frequencies. Then there exists an optimal prefix code for C in which the code words for x and y have the same length and differ only inthe last bit.

Lemma 16.2 implies that the process of building up an optimal tree by mergers can, without loss of generality, begin with the greedy choice of merging together those two characters of lowest frequency. Why is this a greedy choice? We can view the cost of a single merger as being the sum of the frequencies of the two items being merged. Exercise 16.3-3 shows that the total cost of the tree constructed is the sum of the costs of its mergers. Of all possible mergers at each step, HUFFMAN chooses the one that incurs the least cost.

The next lemma shows that the problem of constructing optimal prefix codes has theoptimal-substructure property.
Lemma 16.3 Let Cbe agiven alphabetwith frequency f[c] defined for each character c¡ÊC. Let x and y be two characters in C with minimum frequency. Let C¡ä be the alphabet C with characters x, y removed and (new)character z added, so that C¡ä=C-{x, y}¡È{z}; define f for C¡ä as for C, except that f[z]=f[x]+f[y]. Let T¡ä be any tree representing an optimal prefix code for the alphabet C¡ä .Then the tree T, obtained from T¡ä by replacing the leaf node for z with an internal node having x and yas children, represents an optimal prefix code for the alphabet C.

Theorem 16.4:
Procedure HUFFMAN produces an optimal prefix code.
Proof Immediate from Lemmas 16.2 and 16.3.

16.4 Theoretical foundations for greedy methods:
There is a beautiful theory about greedy algorithms, which wesketch in this section. This theory is useful in determining when the greedy method yields optimal solutions. It involves combinatorial structures known as "matroids." Although this theory does not cover all cases for which a greedy method applies (for example, it does not cover the activity-selection problem of Section 16.1or the Huffman coding problem of Section 16.3), it does cover many cases of practical interest. Furthermore, this theory is being rapidly developed and extended to cover many more applications; see the notes at the end of this chapter for references.

16.5 A task-scheduling problem:
An interesting problem that can be solved using matroids is the problem of optimally scheduling unit-time tasks on a single processor, where each task has adeadline, along with a penalty that must be paid if the deadline is missed. The problem looks complicated, but it can be solved in a surprisingly simple manner using a greedy algorithm.
A unit-time task is a job, such as aprogram to be run on a computer, that requires exactly one unit of time to complete. Given a finite set S of unit-time tasks, a schedule for S is a permutation of S specifying the order in which these tasks are to be performed. The first task in the schedule begins at time 0 and finishes at time 1, the second task begins at time 1 and finishes at time 2, and so on.